{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "print(device)\n",
    "# Define a function to process each example\n",
    "def process_example(example):\n",
    "    # Split the example into IN and OUT parts, and remove the labels\n",
    "    text = example['text']\n",
    "    parts = text.split('OUT:')\n",
    "    in_part = parts[0].replace('IN:', '').strip()\n",
    "    out_part = parts[1].strip() if len(parts) > 1 else ''\n",
    "    return {'input': in_part, 'output': out_part}\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'text', \n",
    "    data_files={'train': 'data/simple_split/tasks_train_simple.txt',\n",
    "                'test': 'data/simple_split/tasks_test_simple.txt'})\n",
    "\n",
    "# # Assuming your processed dataset is stored in a Hugging Face Dataset object called `processed_dataset`\n",
    "# # Get the original samples as a list of dictionaries\n",
    "# original_samples = datasets['train'].to_dict()\n",
    "\n",
    "# # Calculate how many additional samples are needed\n",
    "# total_samples = 100000\n",
    "# original_count = len(original_samples['text'])\n",
    "# additional_count = total_samples - original_count\n",
    "\n",
    "# # Randomly sample additional samples with replacement\n",
    "# additional_samples = {\n",
    "#     key: random.choices(original_samples[key], k=additional_count)\n",
    "#     for key in original_samples\n",
    "# }\n",
    "\n",
    "# # Combine the original samples and the additional samples\n",
    "# datasets['train'] = Dataset.from_dict({\n",
    "#         key: original_samples[key] + additional_samples[key]\n",
    "#         for key in original_samples\n",
    "#     })\n",
    "\n",
    "# # Verify the structure of the combined DatasetDict\n",
    "# print(datasets)\n",
    "\n",
    "datasets['train'] = datasets['train'].map(process_example)\n",
    "datasets['test'] = datasets['test'].map(process_example)\n",
    "# Display the processed dataset\n",
    "print(datasets, datasets['train'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(datasets['train'][0]['text'], \n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=512)\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_function(example):\n",
    "    inputs = dict()\n",
    "    input_str = [example[\"input\"][idx] + tokenizer.sep_token + (' ' + tokenizer.mask_token) * MAX_LENGTH for idx in range(len(example[\"input\"]))]\n",
    "    output_str = [example[\"input\"][idx] + tokenizer.sep_token + example[\"output\"][idx] + (' ' + tokenizer.sep_token) * MAX_LENGTH for idx in range(len(example[\"input\"]))]\n",
    "\n",
    "    input_tokens = tokenizer(input_str, \n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=MAX_LENGTH)\n",
    "    output_tokens = tokenizer(output_str, \n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=MAX_LENGTH)\n",
    "    \n",
    "    inputs.update(input_tokens)\n",
    "    inputs['labels'] = output_tokens['input_ids']\n",
    "    for idx in range(len(inputs['labels'])):\n",
    "        for i in range(len(inputs['labels'][idx])):\n",
    "            sep = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "            if inputs['labels'][idx][i] != sep:\n",
    "                inputs['labels'][idx][i] = -100\n",
    "            else:\n",
    "                inputs['labels'][idx][i] = -100\n",
    "                break\n",
    "\n",
    "\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=['text', 'input', 'output'])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=512, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-cased\", force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# Evaluation\n",
    "\n",
    "def calculate_accuracies(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate token-wise accuracy and sequence-wise accuracy.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list[list]): List of predicted token sequences.\n",
    "        targets (list[list]): List of target token sequences.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (token_wise_accuracy, sequence_wise_accuracy)\n",
    "    \"\"\"\n",
    "    # Ensure predictions and targets are the same length\n",
    "    assert len(predictions) == len(targets), \"Predictions and targets must have the same number of sequences.\"\n",
    "    \n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    correct_sequences = 0\n",
    "    \n",
    "    for pred_seq, target_seq in zip(predictions, targets):\n",
    "        # Ensure sequences are the same length\n",
    "        assert len(pred_seq) == len(target_seq), \"Each prediction and target sequence must have the same length.\"\n",
    "        \n",
    "        # Token-wise comparison\n",
    "        total_tokens += len(target_seq)\n",
    "        correct_tokens += sum(p == t for p, t in zip(pred_seq, target_seq))\n",
    "        \n",
    "        # Sequence-wise comparison\n",
    "        if pred_seq == target_seq:\n",
    "            correct_sequences += 1\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    token_wise_accuracy = correct_tokens / total_tokens\n",
    "    sequence_wise_accuracy = correct_sequences / len(targets)\n",
    "    \n",
    "    return token_wise_accuracy, sequence_wise_accuracy\n",
    "\n",
    "\n",
    "def evaluation(dataset, model, batch_size, tokenizer):\n",
    "    data_loader = DataLoader(dataset, # tokenized_datasets['test'].with_format(\"torch\") \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False)\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    model.to(device)\n",
    "    # print(model.device, device)\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            for key in data:\n",
    "                data[key] = data[key].to(model.device)\n",
    "                # print(key, data[key].device)\n",
    "            outputs = model(**data, return_dict=True)  \n",
    "            prediction = outputs.logits.argmax(dim=-1)\n",
    "            for batch_idx in range(len(data['labels'])):\n",
    "                target = []\n",
    "                pre = []\n",
    "                for i in range(len(data['labels'][batch_idx])):\n",
    "                    if data['labels'][batch_idx][i] != -100:\n",
    "                        sep = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "                        if data['labels'][batch_idx][i] == sep:\n",
    "                            targets.append(target)\n",
    "                            predictions.append(pre)\n",
    "                            break\n",
    "                        target.append(data['labels'][batch_idx][i].cpu().item())\n",
    "                        pre.append(prediction[batch_idx][i].cpu().item())     \n",
    "                            \n",
    "            # print(tokenizer.batch_decode(prediction.tolist(), skip_special_tokens=True))\n",
    "            # print(tokenizer.batch_decode(data['labels'].tolist(), skip_special_tokens=True))\n",
    "\n",
    "            # pprint(predictions)\n",
    "            # pprint(targets)\n",
    "            # pprint(outputs.logits.shape)\n",
    "            # break\n",
    "    return calculate_accuracies(predictions=predictions, targets=targets), predictions, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation(tokenized_datasets['test'].with_format(\"torch\"), \n",
    "                     model, batch_size=1, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token wise ACC:\", results[0][0], \";Sentence wise ACC:\", results[0][1])\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
