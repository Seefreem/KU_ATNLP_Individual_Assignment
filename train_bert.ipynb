{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "print(device)\n",
    "# Define a function to process each example\n",
    "def process_example(example):\n",
    "    # Split the example into IN and OUT parts, and remove the labels\n",
    "    text = example['text']\n",
    "    parts = text.split('OUT:')\n",
    "    in_part = parts[0].replace('IN:', '').strip()\n",
    "    out_part = parts[1].strip() if len(parts) > 1 else ''\n",
    "    return {'input': in_part, 'output': out_part}\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'text', \n",
    "    data_files={'train': 'data/simple_split/tasks_train_simple.txt',\n",
    "                'test': 'data/simple_split/tasks_test_simple.txt'})\n",
    "\n",
    "# # Assuming your processed dataset is stored in a Hugging Face Dataset object called `processed_dataset`\n",
    "# # Get the original samples as a list of dictionaries\n",
    "# original_samples = datasets['train'].to_dict()\n",
    "\n",
    "# # Calculate how many additional samples are needed\n",
    "# total_samples = 100000\n",
    "# original_count = len(original_samples['text'])\n",
    "# additional_count = total_samples - original_count\n",
    "\n",
    "# # Randomly sample additional samples with replacement\n",
    "# additional_samples = {\n",
    "#     key: random.choices(original_samples[key], k=additional_count)\n",
    "#     for key in original_samples\n",
    "# }\n",
    "\n",
    "# # Combine the original samples and the additional samples\n",
    "# datasets['train'] = Dataset.from_dict({\n",
    "#         key: original_samples[key] + additional_samples[key]\n",
    "#         for key in original_samples\n",
    "#     })\n",
    "\n",
    "# # Verify the structure of the combined DatasetDict\n",
    "# print(datasets)\n",
    "\n",
    "datasets['train'] = datasets['train'].map(process_example)\n",
    "datasets['test'] = datasets['test'].map(process_example)\n",
    "# Display the processed dataset\n",
    "print(datasets, datasets['train'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(datasets['train'][0]['text'], \n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=512)\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_function(example):\n",
    "    inputs = dict()\n",
    "    input_str = [example[\"input\"][idx] + tokenizer.sep_token + (' ' + tokenizer.mask_token) * MAX_LENGTH for idx in range(len(example[\"input\"]))]\n",
    "    output_str = [example[\"input\"][idx] + tokenizer.sep_token + example[\"output\"][idx] + (' ' + tokenizer.sep_token) * MAX_LENGTH for idx in range(len(example[\"input\"]))]\n",
    "\n",
    "    input_tokens = tokenizer(input_str, \n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=MAX_LENGTH)\n",
    "    output_tokens = tokenizer(output_str, \n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=MAX_LENGTH)\n",
    "    \n",
    "    inputs.update(input_tokens)\n",
    "    inputs['labels'] = output_tokens['input_ids']\n",
    "    for idx in range(len(inputs['labels'])):\n",
    "        for i in range(len(inputs['labels'][idx])):\n",
    "            sep = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "            if inputs['labels'][idx][i] != sep:\n",
    "                inputs['labels'][idx][i] = -100\n",
    "            else:\n",
    "                inputs['labels'][idx][i] = -100\n",
    "                break\n",
    "\n",
    "\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=['text', 'input', 'output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-cased\", force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# Evaluation\n",
    "\n",
    "def calculate_accuracies(eval_preds):\n",
    "    \"\"\"\n",
    "    Calculate token-wise accuracy and sequence-wise accuracy.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list[list]): List of predicted token sequences.\n",
    "        targets (list[list]): List of target token sequences.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (token_wise_accuracy, sequence_wise_accuracy)\n",
    "    \"\"\"\n",
    "    # Ensure predictions and targets are the same length\n",
    "    logits, labels = eval_preds\n",
    "    assert len(predictions) == len(targets), \"Predictions and targets must have the same number of sequences.\"\n",
    "    prediction = logits.argmax(dim=-1).cpu()\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    \n",
    "    for batch_idx in range(len(labels)):\n",
    "        target = []\n",
    "        pre = []\n",
    "        for i in range(len(labels[batch_idx])):\n",
    "            if labels[batch_idx][i].item() != -100:\n",
    "                sep = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "                if labels[batch_idx][i].item() == sep:\n",
    "                    targets.append(target)\n",
    "                    predictions.append(pre)\n",
    "                    break\n",
    "                target.append(labels[batch_idx][i].cpu().item())\n",
    "                pre.append(prediction[batch_idx][i].cpu().item())     \n",
    "    \n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    correct_sequences = 0\n",
    "    \n",
    "    for pred_seq, target_seq in zip(predictions, targets):\n",
    "        # Ensure sequences are the same length\n",
    "        assert len(pred_seq) == len(target_seq), \"Each prediction and target sequence must have the same length.\"\n",
    "        \n",
    "        # Token-wise comparison\n",
    "        total_tokens += len(target_seq)\n",
    "        correct_tokens += sum(p == t for p, t in zip(pred_seq, target_seq))\n",
    "        \n",
    "        # Sequence-wise comparison\n",
    "        if pred_seq == target_seq:\n",
    "            correct_sequences += 1\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    token_wise_accuracy = correct_tokens / total_tokens\n",
    "    sequence_wise_accuracy = correct_sequences / len(targets)\n",
    "    \n",
    "    return {\"token_wise_accuracy\": token_wise_accuracy,\n",
    "            \"sequence_wise_accuracy\": sequence_wise_accuracy,\n",
    "            \"targets\": targets,\n",
    "            \"predictions\": predictions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetune\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=10,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=512, padding=True)\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=calculate_accuracies,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./bert_finetuned\")\n",
    "tokenizer.save_pretrained(\"./bert_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Token wise ACC:\", results[0][0], \";Sentence wise ACC:\", results[0][1])\n",
    "# print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
